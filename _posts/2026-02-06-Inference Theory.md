---
title: "[Basics]Inference Theory"
categories: ai
tags: [Basic, AI]
layout: single
share: false
---

## Attention

**Attention**: A mechanism that computes how strongly each token in a sequence relates to other tokens in the same sequence.

---

## From Embedding to Contextual Meaning

**Embedding** captures the *intrinsic meaning* of a word, while **Attention layers** transform it into a *context-aware meaning*.

```
Token ‚Üí Embedding (static meaning)
      ‚Üí Attention layers (contextual meaning)
      ‚Üí Contextual vector (true meaning in context)
```

### Example

**0) Starting sentence**

> "The bank is near the river"

---

**1) Token ‚Üí Embedding (no context yet)**
At this stage, each token only contains its independent lexical meaning.

* bank ‚Üí [0.21, -0.44, ...]
* river ‚Üí [0.55, 0.11, ...]

The vector for **bank** does not yet know whether it refers to a financial institution or a riverside.

---

**2) Self-Attention (Layer 1) ‚Üí Attention layer output**
Now tokens begin to interact. The model computes how strongly each token relates to others.

* Attention from **bank ‚Üí river** increases
* The vector of **bank** starts incorporating information related to *river*
* bank ‚Üí *a vector mixed with river-related meaning*

This is where context begins to shape interpretation.

---

**3) Repeated across multiple layers ‚Üí Contextual vector (final meaning)**
As attention layers stack deeper, the representation becomes progressively more refined.

* **Early layers** ‚Üí capture syntax and local structure
* **Middle layers** ‚Üí learn semantic relationships
* **Later layers** ‚Üí form higher-level and abstract meaning

By the final layer, **bank**

---

## FlashAttention

**FlashAttention**: An algorithm that drastically reduces memory reads/writes during attention computation.  
Modern GPUs have extremely fast compute, but memory bandwidth is relatively slower ‚Äî FlashAttention was designed to remove this bottleneck.

### Core Principles

**1. Tiling**  
Split data into small tiles and process them inside fast on-chip GPU memory (SRAM/shared memory).

**2. Recomputation**  
Instead of storing large intermediate results in slow memory, recompute them when needed.  

---

## Multi-Head Attention

Using only a single self-attention mechanism allows the model to capture **one dominant type of relationship** at a time ‚Äî for example, syntax, semantics, or long-range dependency.
However, natural language contains **multiple relationships simultaneously**, and understanding requires modeling them together.

These relationships can be viewed in different ways:

* **Syntactic relationships** ‚Üí subject‚Äìverb agreement, grammatical structure
* **Semantic relationships** ‚Üí synonyms, similar concepts, related meanings
* **Coreference** ‚Üí resolving references such as *‚Äúit‚Äù ‚Üí ‚Äúanimal‚Äù*
* **Long-range dependencies** ‚Üí connections between distant parts of a sentence

To capture these diverse patterns, Transformers use **multiple attention mechanisms in parallel**, known as **Multi-Head Attention**.

### Different Heads, Different Focus

Each attention head learns to specialize in a different type of relationship.

**Example specialization**

* **Head 1** ‚Üí syntactic structure
* **Head 2** ‚Üí semantic similarity
* **Head 3** ‚Üí long-range connections
* **Head 4** ‚Üí coreference resolution

When the outputs of all heads are combined, the model forms a **richer and more expressive representation**, integrating multiple linguistic perspectives at once.

üëâ Multi-Head Attention allows the model to understand language **from several angles simultaneously**, rather than through a single relational lens.

---

## Core Concept
Inside the Transformer, there is one single concept called **Attention**,
and depending on the situation and location, several different forms of Attention are used at the same time.
<br>

> **In other words, they are not separate/different techniques ‚Üí they are simply variations of the same Attention mechanism.**
<br>

| Term                  | Meaning                                                                | Where it is used                                |
| --------------------- | ---------------------------------------------------------------------- | ----------------------------------------------- |
| Self-Attention        | Computes relationships between tokens within the same sequence         | "Encoder, Decoder"                              |
| Masked Self-Attention | Self-Attention that prevents attending to future tokens                | Decoder / GPT                                   |
| Cross-Attention       | Attends to a different sequence (usually the Encoder output)           | Decoder in Encoder‚ÄìDecoder architectures        |
| Multi-Head Attention  | Performs multiple attention operations in parallel                     | All Transformer models                          |
| FlashAttention        | An algorithm that significantly speeds up attention computation        | Implementation / performance optimization level |

